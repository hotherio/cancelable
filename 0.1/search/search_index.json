{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CHANGE-ME","text":""},{"location":"#test","title":"TEST","text":"\\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The <code>range()</code> function is used to generate a sequence of numbers. bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j] # (1)\n</code></pre></p> <ol> <li>:A lot of loops!</li> </ol> <pre><code>theme:\n  features:\n    - content.code.annotate # (1)\n</code></pre> <ol> <li> I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be expressed in Markdown<sup>1</sup>.</li> </ol>"},{"location":"#installation","title":"Installation","text":"WindowsLinux <p>For Windows</p> <p>For Linux</p> Method Description <code>GET</code>      Fetch resource <code>PUT</code>  Update resource <code>DELETE</code>      Delete resource"},{"location":"#test_1","title":"Test","text":"<p><code>mermaid sequenceDiagram   Alice-&gt;&gt;John: Hello John, how are you?   loop Healthcheck       John-&gt;&gt;John: Fight against hypochondria   end   Note right of John: Rational thoughts!   John--&gt;&gt;Alice: Great!   John-&gt;&gt;Bob: How about you?   Bob--&gt;&gt;John: Jolly good!</code></p> <p> </p> Image caption <ol> <li> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.\u00a0\u21a9</p> </li> </ol>"},{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/#core-classes","title":"Core Classes","text":""},{"location":"api_reference/#cancellable","title":"Cancellable","text":"<p>The main class for managing cancellable operations.</p> <pre><code>class Cancellable:\n    def __init__(\n        self,\n        operation_id: Optional[str] = None,\n        name: Optional[str] = None,\n        parent: Optional['Cancellable'] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        register_globally: bool = False,\n    )\n</code></pre>"},{"location":"api_reference/#factory-methods","title":"Factory Methods","text":""},{"location":"api_reference/#with_timeout","title":"with_timeout","text":"<p><pre><code>@classmethod\ndef with_timeout(\n    cls,\n    timeout: Union[float, timedelta],\n    operation_id: Optional[str] = None,\n    name: Optional[str] = None,\n    **kwargs\n) -&gt; 'Cancellable'\n</code></pre> Create a cancellable that times out after the specified duration.</p>"},{"location":"api_reference/#with_token","title":"with_token","text":"<p><pre><code>@classmethod\ndef with_token(\n    cls,\n    token: CancellationToken,\n    operation_id: Optional[str] = None,\n    name: Optional[str] = None,\n    **kwargs\n) -&gt; 'Cancellable'\n</code></pre> Create a cancellable controlled by a cancellation token.</p>"},{"location":"api_reference/#with_signal","title":"with_signal","text":"<p><pre><code>@classmethod\ndef with_signal(\n    cls,\n    *signals: int,\n    operation_id: Optional[str] = None,\n    name: Optional[str] = None,\n    **kwargs\n) -&gt; 'Cancellable'\n</code></pre> Create a cancellable that responds to OS signals.</p>"},{"location":"api_reference/#with_condition","title":"with_condition","text":"<p><pre><code>@classmethod\ndef with_condition(\n    cls,\n    condition: Callable[[], Union[bool, Awaitable[bool]]],\n    check_interval: float = 0.1,\n    condition_name: Optional[str] = None,\n    operation_id: Optional[str] = None,\n    name: Optional[str] = None,\n    **kwargs\n) -&gt; 'Cancellable'\n</code></pre> Create a cancellable that monitors a condition.</p>"},{"location":"api_reference/#methods","title":"Methods","text":""},{"location":"api_reference/#combine","title":"combine","text":"<p><pre><code>def combine(self, *others: 'Cancellable') -&gt; 'Cancellable'\n</code></pre> Combine multiple cancellables into one.</p>"},{"location":"api_reference/#stream","title":"stream","text":"<p><pre><code>async def stream(\n    self,\n    async_iter: AsyncIterator[T],\n    report_interval: Optional[int] = None,\n    buffer_partial: bool = True,\n) -&gt; AsyncIterator[T]\n</code></pre> Wrap an async iterator with cancellation support.</p>"},{"location":"api_reference/#shield","title":"shield","text":"<p><pre><code>@asynccontextmanager\nasync def shield(self) -&gt; AsyncIterator['Cancellable']\n</code></pre> Create a shielded context that won't be cancelled.</p>"},{"location":"api_reference/#cancel","title":"cancel","text":"<p><pre><code>async def cancel(\n    self,\n    reason: CancellationReason = CancellationReason.MANUAL,\n    message: Optional[str] = None,\n    propagate_to_children: bool = True,\n) -&gt; None\n</code></pre> Cancel the operation.</p>"},{"location":"api_reference/#callbacks","title":"Callbacks","text":"<ul> <li><code>on_progress(callback)</code>: Register progress callback</li> <li><code>on_start(callback)</code>: Register start callback</li> <li><code>on_complete(callback)</code>: Register completion callback</li> <li><code>on_cancel(callback)</code>: Register cancellation callback</li> <li><code>on_error(callback)</code>: Register error callback</li> </ul>"},{"location":"api_reference/#cancellationtoken","title":"CancellationToken","text":"<p>Thread-safe token for manual cancellation.</p> <pre><code>class CancellationToken:\n    async def cancel(\n        self,\n        reason: CancellationReason = CancellationReason.MANUAL,\n        message: Optional[str] = None,\n    ) -&gt; bool\n\n    async def wait_for_cancel(self) -&gt; None\n\n    def check(self) -&gt; None  # Raises if cancelled\n\n    async def check_async(self) -&gt; None  # Async version\n\n    def is_cancellation_requested(self) -&gt; bool\n</code></pre>"},{"location":"api_reference/#operationregistry","title":"OperationRegistry","text":"<p>Global registry for operation management.</p> <pre><code>class OperationRegistry:\n    @classmethod\n    def get_instance(cls) -&gt; 'OperationRegistry'\n\n    async def register(self, operation: Cancellable) -&gt; None\n\n    async def get_operation(self, operation_id: str) -&gt; Optional[Cancellable]\n\n    async def list_operations(\n        self,\n        status: Optional[OperationStatus] = None,\n        parent_id: Optional[str] = None,\n        name_pattern: Optional[str] = None,\n    ) -&gt; List[OperationContext]\n\n    async def cancel_operation(\n        self,\n        operation_id: str,\n        reason: CancellationReason = CancellationReason.MANUAL,\n        message: Optional[str] = None,\n    ) -&gt; bool\n</code></pre>"},{"location":"api_reference/#enums","title":"Enums","text":""},{"location":"api_reference/#operationstatus","title":"OperationStatus","text":"<ul> <li><code>PENDING</code>: Operation created but not started</li> <li><code>RUNNING</code>: Operation is currently executing</li> <li><code>COMPLETED</code>: Operation completed successfully</li> <li><code>CANCELLED</code>: Operation was cancelled</li> <li><code>FAILED</code>: Operation failed with error</li> <li><code>TIMEOUT</code>: Operation timed out</li> <li><code>SHIELDED</code>: Operation is in shielded section</li> </ul>"},{"location":"api_reference/#cancellationreason","title":"CancellationReason","text":"<ul> <li><code>TIMEOUT</code>: Cancelled due to timeout</li> <li><code>MANUAL</code>: Cancelled manually via token or API</li> <li><code>SIGNAL</code>: Cancelled by OS signal</li> <li><code>CONDITION</code>: Cancelled by condition check</li> <li><code>PARENT</code>: Cancelled because parent was cancelled</li> <li><code>ERROR</code>: Cancelled due to error</li> </ul>"},{"location":"api_reference/#decorators","title":"Decorators","text":""},{"location":"api_reference/#cancellable_1","title":"@cancellable","text":"<p><pre><code>@cancellable(\n    timeout: Optional[Union[float, timedelta]] = None,\n    operation_id: Optional[str] = None,\n    name: Optional[str] = None,\n    register_globally: bool = False,\n    inject_param: Optional[str] = \"cancellable\",\n)\n</code></pre> Decorator to make async functions cancellable.</p>"},{"location":"api_reference/#cancellable_method","title":"@cancellable_method","text":"<p><pre><code>@cancellable_method(\n    timeout: Optional[Union[float, timedelta]] = None,\n    name: Optional[str] = None,\n    register_globally: bool = False,\n)\n</code></pre> Decorator for async methods (automatically includes class name).</p>"},{"location":"api_reference/#utility-functions","title":"Utility Functions","text":""},{"location":"api_reference/#with_timeout_1","title":"with_timeout","text":"<p><pre><code>async def with_timeout(\n    timeout: Union[float, timedelta],\n    coro: Awaitable[T],\n    operation_id: Optional[str] = None,\n    name: Optional[str] = None,\n) -&gt; T\n</code></pre> Run a coroutine with timeout.</p>"},{"location":"api_reference/#cancellable_stream","title":"cancellable_stream","text":"<p><pre><code>async def cancellable_stream(\n    stream: AsyncIterator[T],\n    timeout: Optional[Union[float, timedelta]] = None,\n    token: Optional[CancellationToken] = None,\n    report_interval: Optional[int] = None,\n    on_progress: Optional[Callable[[int, T], Any]] = None,\n    buffer_partial: bool = False,\n    operation_id: Optional[str] = None,\n    name: Optional[str] = None,\n) -&gt; AsyncIterator[T]\n</code></pre> Make any async iterator cancellable.</p>"},{"location":"api_reference/#exceptions","title":"Exceptions","text":"<ul> <li><code>CancellationError</code>: Base exception for cancellation</li> <li><code>TimeoutCancellation</code>: Operation timed out</li> <li><code>ManualCancellation</code>: Operation cancelled manually</li> <li><code>SignalCancellation</code>: Operation cancelled by signal</li> <li><code>ConditionCancellation</code>: Operation cancelled by condition</li> <li><code>ParentCancellation</code>: Operation cancelled by parent</li> </ul>"},{"location":"getting_started/","title":"Getting Started with Async Cancellation","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>The async cancellation system is part of the Forge framework. Ensure you have the required dependencies:</p> <pre><code>uv pip install anyio asyncer structlog pydantic httpx\n</code></pre>"},{"location":"getting_started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"getting_started/#cancellable-operations","title":"Cancellable Operations","text":"<p>A <code>Cancellable</code> provides a context for managing async operations that can be cancelled from various sources:</p> <pre><code>from forge.async_cancellation import Cancellable\n\nasync with Cancellable() as cancel:\n    # Your async operation here\n    result = await some_async_operation()\n</code></pre>"},{"location":"getting_started/#cancellation-sources","title":"Cancellation Sources","text":"<p>Operations can be cancelled from multiple sources:</p> <ol> <li>Timeout: Cancel after a specified duration</li> <li>Token: Manual cancellation via a token</li> <li>Signal: OS signal handling (SIGINT, etc.)</li> <li>Condition: Custom condition checking</li> </ol>"},{"location":"getting_started/#quick-examples","title":"Quick Examples","text":""},{"location":"getting_started/#timeout-cancellation","title":"Timeout Cancellation","text":"<pre><code>from forge.async_cancellation import Cancellable\nfrom datetime import timedelta\n\n# Using seconds\nasync with Cancellable.with_timeout(30.0) as cancel:\n    result = await long_running_operation()\n\n# Using timedelta\nasync with Cancellable.with_timeout(timedelta(minutes=5)) as cancel:\n    result = await very_long_operation()\n</code></pre>"},{"location":"getting_started/#manual-cancellation","title":"Manual Cancellation","text":"<pre><code>from forge.async_cancellation import Cancellable, CancellationToken\n\n# Create a token\ntoken = CancellationToken()\n\n# In your async operation\nasync def my_operation():\n    async with Cancellable.with_token(token) as cancel:\n        # This will be cancelled when token.cancel() is called\n        await some_work()\n\n# Cancel from elsewhere\nawait token.cancel()\n</code></pre>"},{"location":"getting_started/#progress-reporting","title":"Progress Reporting","text":"<pre><code>async with Cancellable() as cancel:\n    # Register progress callback\n    cancel.on_progress(lambda op_id, msg, meta: print(f\"Progress: {msg}\"))\n\n    # Report progress during operation\n    await cancel.report_progress(\"Starting operation\")\n\n    for i in range(100):\n        await process_item(i)\n        if i % 10 == 0:\n            await cancel.report_progress(f\"Processed {i} items\", {\"count\": i})\n</code></pre>"},{"location":"getting_started/#stream-processing","title":"Stream Processing","text":"<pre><code>async with Cancellable.with_timeout(60) as cancel:\n    # Process async stream with automatic cancellation\n    async for item in cancel.stream(async_data_source()):\n        await process_item(item)\n</code></pre>"},{"location":"getting_started/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always use context managers: Ensures proper cleanup    <pre><code>async with Cancellable() as cancel:\n    # Your code here\n</code></pre></p> </li> <li> <p>Report progress for long operations: Helps with monitoring    <pre><code>await cancel.report_progress(\"Processing batch\", {\"size\": len(batch)})\n</code></pre></p> </li> <li> <p>Handle cancellation gracefully: Save partial results    <pre><code>try:\n    async with Cancellable.with_timeout(30) as cancel:\n        result = await process_all()\nexcept Exception:\n    # Save partial results from cancel.context.partial_result\n    pass\n</code></pre></p> </li> <li> <p>Use appropriate cancellation sources: Choose based on your needs</p> </li> <li>Timeout: For operations with known maximum duration</li> <li>Token: For user-initiated cancellation</li> <li>Signal: For system-level interruption</li> <li>Condition: For resource-based cancellation</li> </ol>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Read the API Reference for detailed documentation</li> <li>Check out Common Patterns for advanced usage</li> <li>See the <code>examples/</code> directory for complete examples</li> </ul>"},{"location":"migrating_guide/","title":"Migration Guide","text":""},{"location":"migrating_guide/#migrating-from-asynciotimeout","title":"Migrating from asyncio.timeout","text":"<p>If you're currently using <code>asyncio.timeout</code> (Python 3.11+):</p>"},{"location":"migrating_guide/#before","title":"Before:","text":"<pre><code>import asyncio\n\nasync with asyncio.timeout(30):\n    result = await long_operation()\n</code></pre>"},{"location":"migrating_guide/#after","title":"After:","text":"<pre><code>from forge.async_cancellation import Cancellable\n\nasync with Cancellable.with_timeout(30) as cancel:\n    result = await long_operation()\n</code></pre>"},{"location":"migrating_guide/#benefits","title":"Benefits:","text":"<ul> <li>Progress reporting</li> <li>Multiple cancellation sources</li> <li>Better error handling</li> <li>Operation tracking</li> </ul>"},{"location":"migrating_guide/#migrating-from-anyiocancelscope","title":"Migrating from anyio.CancelScope","text":"<p>If you're using anyio's cancel scopes directly:</p>"},{"location":"migrating_guide/#before_1","title":"Before:","text":"<pre><code>import anyio\n\nwith anyio.CancelScope() as scope:\n    scope.deadline = anyio.current_time() + 30\n    await operation()\n</code></pre>"},{"location":"migrating_guide/#after_1","title":"After:","text":"<pre><code>from forge.async_cancellation import Cancellable\n\nasync with Cancellable.with_timeout(30) as cancel:\n    await operation()\n</code></pre>"},{"location":"migrating_guide/#benefits_1","title":"Benefits:","text":"<ul> <li>Higher-level API</li> <li>Built-in progress tracking</li> <li>Automatic cleanup</li> <li>Integration with other libraries</li> </ul>"},{"location":"migrating_guide/#migrating-manual-cancellation","title":"Migrating Manual Cancellation","text":"<p>If you have manual cancellation patterns:</p>"},{"location":"migrating_guide/#before_2","title":"Before:","text":"<pre><code>class Worker:\n    def __init__(self):\n        self.should_stop = False\n\n    async def run(self):\n        while not self.should_stop:\n            await self.do_work()\n\n    def stop(self):\n        self.should_stop = True\n</code></pre>"},{"location":"migrating_guide/#after_2","title":"After:","text":"<pre><code>from forge.async_cancellation import CancellationToken\n\nclass Worker:\n    def __init__(self):\n        self.token = CancellationToken()\n\n    async def run(self):\n        while True:\n            await self.token.check_async()\n            await self.do_work()\n\n    async def stop(self):\n        await self.token.cancel()\n</code></pre>"},{"location":"migrating_guide/#benefits_2","title":"Benefits:","text":"<ul> <li>Thread-safe cancellation</li> <li>Proper async/await support</li> <li>Cancellation callbacks</li> <li>Integration with Cancellable</li> </ul>"},{"location":"migrating_guide/#migrating-stream-processing","title":"Migrating Stream Processing","text":"<p>If you have custom stream cancellation:</p>"},{"location":"migrating_guide/#before_3","title":"Before:","text":"<pre><code>async def process_stream(stream):\n    try:\n        async for item in stream:\n            if should_stop():\n                break\n            await process_item(item)\n    except asyncio.CancelledError:\n        # Handle cancellation\n        pass\n</code></pre>"},{"location":"migrating_guide/#after_3","title":"After:","text":"<pre><code>from forge.async_cancellation import Cancellable\n\nasync def process_stream(stream):\n    async with Cancellable() as cancel:\n        async for item in cancel.stream(stream):\n            await process_item(item)\n</code></pre>"},{"location":"migrating_guide/#benefits_3","title":"Benefits:","text":"<ul> <li>Automatic cancellation propagation</li> <li>Progress reporting</li> <li>Partial result preservation</li> <li>Clean error handling</li> </ul>"},{"location":"migrating_guide/#adding-to-existing-code","title":"Adding to Existing Code","text":"<p>You can gradually adopt the cancellation system:</p>"},{"location":"migrating_guide/#step-1-add-to-critical-operations","title":"Step 1: Add to Critical Operations","text":"<p>Start with operations that need timeouts:</p> <pre><code># Before\nresult = await critical_operation()\n\n# After\nfrom forge.async_cancellation import with_timeout\nresult = await with_timeout(30.0, critical_operation())\n</code></pre>"},{"location":"migrating_guide/#step-2-add-progress-reporting","title":"Step 2: Add Progress Reporting","text":"<p>Enhance long-running operations:</p> <pre><code>from forge.async_cancellation import cancellable\n\n@cancellable(timeout=300)\nasync def process_data(data: list, cancellable=None):\n    for i, item in enumerate(data):\n        await process_item(item)\n        if i % 100 == 0:\n            await cancellable.report_progress(f\"Processed {i} items\")\n</code></pre>"},{"location":"migrating_guide/#step-3-integrate-with-your-framework","title":"Step 3: Integrate with Your Framework","text":"<p>For web applications:</p> <pre><code># FastAPI example\nfrom forge.async_cancellation.integrations.fastapi import cancellable_dependency\n\n@app.post(\"/process\")\nasync def process_endpoint(\n    data: List[str],\n    cancel: Cancellable = Depends(cancellable_dependency)\n):\n    async with cancel:\n        return await process_data(data)\n</code></pre>"},{"location":"migrating_guide/#common-gotchas","title":"Common Gotchas","text":""},{"location":"migrating_guide/#1-context-manager-scope","title":"1. Context Manager Scope","text":"<p>Remember that cancellation is scoped to the context manager:</p> <pre><code># \u274c Wrong: cancellable out of scope\ncancel = Cancellable()\nawait operation()  # Not cancellable!\n\n# \u2705 Correct: use context manager\nasync with Cancellable() as cancel:\n    await operation()  # Cancellable\n</code></pre>"},{"location":"migrating_guide/#2-shielding-side-effects","title":"2. Shielding Side Effects","text":"<p>Be careful with shielding:</p> <pre><code># \u274c Wrong: entire operation shielded\nasync with cancel.shield():\n    await long_operation()  # Won't be cancelled!\n\n# \u2705 Correct: shield only critical parts\nawait long_operation()\nasync with cancel.shield():\n    await critical_cleanup()  # Protected from cancellation\n</code></pre>"},{"location":"migrating_guide/#3-token-lifecycle","title":"3. Token Lifecycle","text":"<p>Tokens can be reused but not reset:</p> <pre><code>token = CancellationToken()\n\n# First use\nasync with Cancellable.with_token(token):\n    await operation1()\n\n# Token can be cancelled later\nawait token.cancel()\n\n# \u274c Wrong: token is already cancelled\nasync with Cancellable.with_token(token):\n    await operation2()  # Will be immediately cancelled!\n\n# \u2705 Correct: create new token\nnew_token = CancellationToken()\nasync with Cancellable.with_token(new_token):\n    await operation2()\n</code></pre>"},{"location":"migrating_guide/#performance-considerations","title":"Performance Considerations","text":"<p>The cancellation system adds minimal overhead:</p> <ul> <li>Context manager: ~10-50% overhead vs raw async</li> <li>Cancellation checks: &lt;10\u03bcs per check</li> <li>Stream processing: &lt;100% overhead vs direct iteration</li> </ul> <p>For most applications, this overhead is negligible compared to I/O operations.</p>"},{"location":"migrating_guide/#getting-help","title":"Getting Help","text":"<ul> <li>Check the examples/ directory for complete examples</li> <li>Read the API Reference for detailed documentation</li> <li>Review Common Patterns for best practices</li> </ul>"},{"location":"patterns/","title":"Common Patterns and Best Practices","text":""},{"location":"patterns/#pattern-graceful-shutdown","title":"Pattern: Graceful Shutdown","text":"<p>Handle application shutdown gracefully:</p> <pre><code>import signal\nfrom forge.async_cancellation import Cancellable\n\nasync def main():\n    # Handle SIGINT and SIGTERM\n    async with Cancellable.with_signal(signal.SIGINT, signal.SIGTERM) as cancel:\n        cancel.on_cancel(lambda ctx: print(\"Shutting down gracefully...\"))\n\n        # Your application logic\n        await run_application()\n</code></pre>"},{"location":"patterns/#pattern-resource-cleanup","title":"Pattern: Resource Cleanup","text":"<p>Ensure resources are cleaned up even on cancellation:</p> <pre><code>async def process_with_cleanup():\n    resource = None\n\n    async with Cancellable.with_timeout(30) as cancel:\n        try:\n            # Acquire resource\n            resource = await acquire_resource()\n\n            # Process\n            result = await process(resource)\n\n            return result\n\n        finally:\n            # Shield cleanup from cancellation\n            if resource:\n                async with cancel.shield():\n                    await resource.cleanup()\n</code></pre>"},{"location":"patterns/#pattern-batch-processing-with-progress","title":"Pattern: Batch Processing with Progress","text":"<p>Process data in batches with progress reporting:</p> <pre><code>async def process_large_dataset(data: List[Any], batch_size: int = 100):\n    async with Cancellable(name=\"batch_processing\") as cancel:\n        cancel.on_progress(lambda op_id, msg, meta: logger.info(msg, **meta))\n\n        total = len(data)\n        processed = 0\n\n        for i in range(0, total, batch_size):\n            batch = data[i:i + batch_size]\n\n            # Process batch\n            await process_batch(batch)\n            processed += len(batch)\n\n            # Report progress\n            progress = (processed / total) * 100\n            await cancel.report_progress(\n                f\"Processed {processed}/{total} items\",\n                {\"progress_percent\": progress, \"batch_number\": i // batch_size + 1}\n            )\n</code></pre>"},{"location":"patterns/#pattern-retry-with-cancellation","title":"Pattern: Retry with Cancellation","text":"<p>Implement retry logic with cancellation support:</p> <pre><code>async def retry_with_cancellation(\n    operation: Callable,\n    max_retries: int = 3,\n    delay: float = 1.0,\n    backoff: float = 2.0,\n):\n    async with Cancellable(name=\"retry_operation\") as cancel:\n        last_error = None\n\n        for attempt in range(max_retries):\n            try:\n                # Check cancellation before retry\n                await cancel._token.check_async()\n\n                # Attempt operation\n                result = await operation()\n                return result\n\n            except Exception as e:\n                last_error = e\n\n                if attempt &lt; max_retries - 1:\n                    await cancel.report_progress(\n                        f\"Attempt {attempt + 1} failed, retrying...\",\n                        {\"error\": str(e)}\n                    )\n\n                    # Wait with exponential backoff\n                    await anyio.sleep(delay)\n                    delay *= backoff\n\n        raise last_error\n</code></pre>"},{"location":"patterns/#pattern-concurrent-operations-with-shared-cancellation","title":"Pattern: Concurrent Operations with Shared Cancellation","text":"<p>Run multiple operations with shared cancellation:</p> <pre><code>async def parallel_operations():\n    async with Cancellable(name=\"parallel_work\") as cancel:\n        async def worker(worker_id: int, items: List[Any]):\n            for item in items:\n                # Check cancellation\n                await cancel._token.check_async()\n\n                # Process item\n                await process_item(item)\n\n                # Report progress\n                await cancel.report_progress(\n                    f\"Worker {worker_id} processed item\",\n                    {\"worker_id\": worker_id, \"item\": item}\n                )\n\n        # Split work among workers\n        work_items = split_into_chunks(all_items, worker_count=4)\n\n        # Run workers concurrently\n        async with anyio.create_task_group() as tg:\n            for i, items in enumerate(work_items):\n                tg.start_soon(worker, i, items)\n</code></pre>"},{"location":"patterns/#pattern-hierarchical-cancellation","title":"Pattern: Hierarchical Cancellation","text":"<p>Create operation hierarchies with parent-child relationships:</p> <pre><code>async def hierarchical_operations():\n    async with Cancellable(name=\"parent_operation\") as parent:\n        # Create child operations\n        async def child_operation(child_id: int):\n            child = Cancellable(\n                name=f\"child_{child_id}\",\n                parent=parent\n            )\n\n            async with child:\n                # Child will be cancelled if parent is cancelled\n                await do_child_work()\n\n        # Run children\n        async with anyio.create_task_group() as tg:\n            for i in range(5):\n                tg.start_soon(child_operation, i)\n</code></pre>"},{"location":"patterns/#pattern-conditional-cancellation","title":"Pattern: Conditional Cancellation","text":"<p>Cancel based on system resources:</p> <pre><code>import psutil\n\ndef check_memory_usage():\n    \"\"\"Cancel if memory usage is too high.\"\"\"\n    return psutil.virtual_memory().percent &gt; 90\n\nasync def memory_aware_operation():\n    async with Cancellable.with_condition(\n        check_memory_usage,\n        check_interval=5.0,\n        condition_name=\"memory_check\"\n    ) as cancel:\n        cancel.on_cancel(\n            lambda ctx: logger.warning(\"Operation cancelled due to high memory usage\")\n        )\n\n        await memory_intensive_operation()\n</code></pre>"},{"location":"patterns/#pattern-stream-processing-with-backpressure","title":"Pattern: Stream Processing with Backpressure","text":"<p>Handle backpressure in stream processing:</p> <pre><code>async def process_stream_with_backpressure(source: AsyncIterator[Any]):\n    # Create bounded channel for backpressure\n    send_channel, receive_channel = anyio.create_memory_object_stream(max_buffer_size=100)\n\n    async with Cancellable(name=\"stream_processing\") as cancel:\n        async def producer():\n            async with send_channel:\n                async for item in cancel.stream(source):\n                    try:\n                        # Try to send without blocking\n                        send_channel.send_nowait(item)\n                    except anyio.WouldBlock:\n                        # Handle backpressure\n                        await cancel.report_progress(\n                            \"Backpressure detected, waiting for consumer\"\n                        )\n                        await send_channel.send(item)\n\n        async def consumer():\n            async with receive_channel:\n                async for item in receive_channel:\n                    # Process item\n                    await process_item(item)\n\n        # Run producer and consumer concurrently\n        async with anyio.create_task_group() as tg:\n            tg.start_soon(producer)\n            tg.start_soon(consumer)\n</code></pre>"},{"location":"patterns/#anti-patterns-to-avoid","title":"Anti-Patterns to Avoid","text":""},{"location":"patterns/#dont-forget-to-check-cancellation-in-loops","title":"Don't forget to check cancellation in loops","text":"<p>\u274c Bad: <pre><code>async with Cancellable.with_timeout(10) as cancel:\n    for item in large_list:\n        # This might run forever if list is large\n        await process_item(item)\n</code></pre></p> <p>\u2705 Good: <pre><code>async with Cancellable.with_timeout(10) as cancel:\n    for item in large_list:\n        # Check cancellation in each iteration\n        await cancel._token.check_async()\n        await process_item(item)\n</code></pre></p>"},{"location":"patterns/#dont-ignore-cancellation-in-cleanup","title":"Don't ignore cancellation in cleanup","text":"<p>\u274c Bad: <pre><code>try:\n    async with Cancellable.with_timeout(10) as cancel:\n        result = await operation()\nfinally:\n    # This might not run if cancelled\n    await cleanup()\n</code></pre></p> <p>\u2705 Good: <pre><code>async with Cancellable.with_timeout(10) as cancel:\n    try:\n        result = await operation()\n    finally:\n        # Shield cleanup from cancellation\n        async with cancel.shield():\n            await cleanup()\n</code></pre></p>"},{"location":"patterns/#dont-create-unnecessary-cancellables","title":"Don't create unnecessary cancellables","text":"<p>\u274c Bad: <pre><code># Creating new cancellable for each item\nfor item in items:\n    async with Cancellable() as cancel:\n        await process_item(item)\n</code></pre></p> <p>\u2705 Good: <pre><code># Reuse single cancellable\nasync with Cancellable() as cancel:\n    for item in items:\n        await process_item(item)\n</code></pre></p>"},{"location":"examples/basic_usage/","title":"Basic Usage Examples","text":"<p>This guide covers the fundamental usage patterns of the cancelable library.</p>"},{"location":"examples/basic_usage/#getting-started","title":"Getting Started","text":""},{"location":"examples/basic_usage/#installation","title":"Installation","text":"<pre><code>uv add cancelable\n</code></pre>"},{"location":"examples/basic_usage/#basic-imports","title":"Basic Imports","text":"<pre><code>from cancelable import (\n    Cancellable,\n    CancellationToken,\n    cancellable,\n    with_timeout,\n    OperationStatus,\n    CancellationReason,\n)\n</code></pre>"},{"location":"examples/basic_usage/#timeout-based-cancellation","title":"Timeout-Based Cancellation","text":""},{"location":"examples/basic_usage/#simple-timeout","title":"Simple Timeout","text":"<pre><code>import asyncio\nfrom cancelable import Cancellable\n\nasync def long_operation():\n    \"\"\"Simulate a long-running operation.\"\"\"\n    await asyncio.sleep(5)\n    return \"Operation completed\"\n\nasync def main():\n    try:\n        # This will timeout after 2 seconds\n        async with Cancellable.with_timeout(2.0) as cancel:\n            result = await long_operation()\n            print(result)  # This won't be reached\n    except asyncio.CancelledError:\n        print(\"Operation timed out!\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/basic_usage/#timeout-with-progress-reporting","title":"Timeout with Progress Reporting","text":"<pre><code>async def operation_with_progress():\n    async with Cancellable.with_timeout(10.0) as cancel:\n        # Add progress callback\n        cancel.on_progress(\n            lambda op_id, msg, meta: print(f\"Progress: {msg}\")\n        )\n\n        for i in range(5):\n            await cancel.report_progress(\n                f\"Step {i+1}/5 completed\",\n                {\"step\": i+1, \"total\": 5}\n            )\n            await asyncio.sleep(1)\n\n        return \"All steps completed\"\n</code></pre>"},{"location":"examples/basic_usage/#manual-cancellation","title":"Manual Cancellation","text":""},{"location":"examples/basic_usage/#using-cancellationtoken","title":"Using CancellationToken","text":"<pre><code>async def cancellable_download(url: str, token: CancellationToken):\n    \"\"\"Download that can be cancelled manually.\"\"\"\n    async with Cancellable.with_token(token) as cancel:\n        cancel.on_start(lambda ctx: print(f\"Starting download: {url}\"))\n        cancel.on_cancel(lambda ctx: print(f\"Download cancelled: {ctx.cancel_reason}\"))\n\n        # Simulate download\n        for i in range(10):\n            await asyncio.sleep(0.5)\n            print(f\"Downloaded {(i+1)*10}%\")\n\n        return f\"Downloaded {url}\"\n\nasync def main():\n    # Create a cancellation token\n    token = CancellationToken()\n\n    # Start download in background\n    download_task = asyncio.create_task(\n        cancellable_download(\"https://example.com/file.zip\", token)\n    )\n\n    # Cancel after 2 seconds\n    await asyncio.sleep(2)\n    await token.cancel(\"User requested cancellation\")\n\n    # Wait for task to handle cancellation\n    try:\n        result = await download_task\n    except asyncio.CancelledError:\n        print(\"Download was cancelled\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/basic_usage/#parent-child-cancellation","title":"Parent-Child Cancellation","text":"<pre><code>async def parent_operation():\n    async with Cancellable.with_timeout(10.0) as parent:\n        print(\"Parent operation started\")\n\n        # Create child operation\n        async with Cancellable(parent=parent) as child:\n            print(\"Child operation started\")\n\n            # If parent is cancelled, child is also cancelled\n            await asyncio.sleep(5)\n\n            print(\"Child operation completed\")\n\n        print(\"Parent operation completed\")\n</code></pre>"},{"location":"examples/basic_usage/#function-decorators","title":"Function Decorators","text":""},{"location":"examples/basic_usage/#basic-decorator-usage","title":"Basic Decorator Usage","text":"<pre><code>from cancelable import cancellable\n\n@cancellable(timeout=5.0)\nasync def process_data(items: list, cancellable: Cancellable = None):\n    \"\"\"Process items with automatic timeout.\"\"\"\n    results = []\n\n    for i, item in enumerate(items):\n        # Report progress\n        await cancellable.report_progress(\n            f\"Processing item {i+1}/{len(items)}\",\n            {\"current\": i+1, \"total\": len(items)}\n        )\n\n        # Process item\n        result = await process_item(item)\n        results.append(result)\n\n    return results\n\n# Usage\nasync def main():\n    items = [1, 2, 3, 4, 5]\n    try:\n        results = await process_data(items)\n        print(f\"Processed {len(results)} items\")\n    except asyncio.CancelledError:\n        print(\"Processing timed out\")\n</code></pre>"},{"location":"examples/basic_usage/#decorator-with-custom-configuration","title":"Decorator with Custom Configuration","text":"<pre><code>@cancellable(\n    timeout=30.0,\n    register_globally=True,  # Register with global registry\n    name=\"data_processor\"\n)\nasync def complex_processing(data: dict, cancellable: Cancellable = None):\n    # Set up callbacks\n    cancellable.on_cancel(\n        lambda ctx: logger.warning(f\"Processing cancelled: {ctx.cancel_reason}\")\n    )\n\n    # Process data\n    return await perform_analysis(data)\n</code></pre>"},{"location":"examples/basic_usage/#combining-cancellation-sources","title":"Combining Cancellation Sources","text":""},{"location":"examples/basic_usage/#multiple-cancellation-conditions","title":"Multiple Cancellation Conditions","text":"<pre><code>import signal\n\nasync def multi_cancellable_operation():\n    # Create individual cancellables\n    timeout_cancel = Cancellable.with_timeout(30.0)\n    signal_cancel = Cancellable.with_signal(signal.SIGINT)\n    token = CancellationToken()\n    manual_cancel = Cancellable.with_token(token)\n\n    # Combine them - cancels if ANY condition is met\n    combined = timeout_cancel.combine(signal_cancel, manual_cancel)\n\n    async with combined as cancel:\n        cancel.on_cancel(\n            lambda ctx: print(f\"Cancelled due to: {ctx.cancel_reason}\")\n        )\n\n        # Long running operation\n        await long_computation()\n</code></pre>"},{"location":"examples/basic_usage/#conditional-cancellation","title":"Conditional Cancellation","text":"<pre><code># Global flag that can be set by other parts of the application\nshould_stop = False\n\nasync def conditional_operation():\n    async with Cancellable.with_condition(\n        lambda: should_stop,  # Check condition\n        check_interval=0.5,   # Check every 0.5 seconds\n        condition_name=\"stop_flag\"\n    ) as cancel:\n        # Operation continues until should_stop becomes True\n        while True:\n            await do_work()\n            await asyncio.sleep(1)\n</code></pre>"},{"location":"examples/basic_usage/#stream-processing","title":"Stream Processing","text":""},{"location":"examples/basic_usage/#basic-stream-cancellation","title":"Basic Stream Cancellation","text":"<pre><code>async def process_stream():\n    async with Cancellable.with_timeout(60.0) as cancel:\n        # Wrap any async iterator\n        async for item in cancel.stream(\n            fetch_items(),  # Your async iterator\n            report_interval=10  # Report every 10 items\n        ):\n            # Process each item\n            result = await process_item(item)\n\n            # Cancellation is checked automatically between items\n</code></pre>"},{"location":"examples/basic_usage/#stream-with-buffering","title":"Stream with Buffering","text":"<pre><code>async def fetch_items():\n    \"\"\"Async generator that yields items.\"\"\"\n    for i in range(100):\n        await asyncio.sleep(0.1)\n        yield f\"item_{i}\"\n\nasync def buffered_stream_processing():\n    async with Cancellable.with_timeout(10.0) as cancel:\n        try:\n            async for item in cancel.stream(\n                fetch_items(),\n                buffer_partial=True  # Keep partial results\n            ):\n                print(f\"Processing {item}\")\n\n        except asyncio.CancelledError:\n            # Access partial results\n            partial = cancel.context.partial_result\n            print(f\"Processed {partial['count']} items before cancellation\")\n            print(f\"Last items: {partial['buffer'][-5:]}\")\n</code></pre>"},{"location":"examples/basic_usage/#error-handling","title":"Error Handling","text":""},{"location":"examples/basic_usage/#handling-different-cancellation-types","title":"Handling Different Cancellation Types","text":"<pre><code>from cancelable import (\n    TimeoutCancellation,\n    ManualCancellation,\n    SignalCancellation,\n)\n\nasync def robust_operation():\n    try:\n        async with Cancellable.with_timeout(30.0) as cancel:\n            result = await do_work()\n            return result\n\n    except TimeoutCancellation as e:\n        logger.error(f\"Operation timed out: {e.message}\")\n        # Handle timeout specifically\n        return None\n\n    except ManualCancellation as e:\n        logger.info(f\"Operation cancelled by user: {e.message}\")\n        # Handle manual cancellation\n        return None\n\n    except SignalCancellation as e:\n        logger.warning(f\"Operation cancelled by signal: {e.signal}\")\n        # Handle signal cancellation\n        return None\n\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n        raise\n</code></pre>"},{"location":"examples/basic_usage/#shielding-critical-operations","title":"Shielding Critical Operations","text":""},{"location":"examples/basic_usage/#protecting-cleanup-code","title":"Protecting Cleanup Code","text":"<pre><code>async def operation_with_cleanup():\n    resource = None\n\n    async with Cancellable.with_timeout(10.0) as cancel:\n        try:\n            # Acquire resource\n            resource = await acquire_resource()\n\n            # Do work (can be cancelled)\n            result = await process_with_resource(resource)\n\n            return result\n\n        finally:\n            # Shield cleanup from cancellation\n            if resource:\n                async with cancel.shield():\n                    # This will complete even if cancelled\n                    await release_resource(resource)\n                    print(\"Resource cleaned up successfully\")\n</code></pre>"},{"location":"examples/basic_usage/#global-operation-registry","title":"Global Operation Registry","text":""},{"location":"examples/basic_usage/#monitoring-active-operations","title":"Monitoring Active Operations","text":"<pre><code>from cancelable import OperationRegistry\n\nasync def monitored_operation():\n    # Register globally\n    async with Cancellable.with_timeout(\n        60.0,\n        name=\"data_import\",\n        register_globally=True\n    ) as cancel:\n        await import_data()\n\nasync def monitor_operations():\n    registry = OperationRegistry.get_instance()\n\n    # List all running operations\n    operations = await registry.list_operations(\n        status=OperationStatus.RUNNING\n    )\n\n    for op in operations:\n        print(f\"Running: {op.name} (ID: {op.id[:8]})\")\n        print(f\"  Started: {op.start_time}\")\n        print(f\"  Duration: {op.duration}s\")\n\n    # Get statistics\n    stats = await registry.get_statistics()\n    print(f\"Total operations: {stats.total_operations}\")\n    print(f\"Currently running: {stats.running}\")\n</code></pre>"},{"location":"examples/basic_usage/#best-practices","title":"Best Practices","text":"<ol> <li>Always use async with: Ensures proper cleanup</li> <li>Set reasonable timeouts: Based on expected operation duration</li> <li>Report progress: For long operations, keep users informed</li> <li>Handle cancellation gracefully: Clean up resources properly</li> <li>Use appropriate cancellation source: Timeout, manual, signal, or condition</li> <li>Combine sources when needed: For flexible cancellation strategies</li> <li>Shield critical operations: Protect cleanup code from cancellation</li> </ol>"},{"location":"examples/basic_usage/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Stream Processing for advanced streaming patterns</li> <li>Check out the Integration Guides for library-specific usage</li> <li>Review Patterns for advanced usage patterns</li> </ul>"},{"location":"examples/stream_processing/","title":"Stream Processing Examples","text":"<p>This guide demonstrates advanced stream processing patterns with the cancelable library.</p>"},{"location":"examples/stream_processing/#overview","title":"Overview","text":"<p>Stream processing with cancellation support is crucial for handling large datasets, real-time data feeds, and long-running data pipelines. The cancelable library provides robust patterns for these scenarios.</p>"},{"location":"examples/stream_processing/#basic-stream-processing","title":"Basic Stream Processing","text":""},{"location":"examples/stream_processing/#simple-stream-cancellation","title":"Simple Stream Cancellation","text":"<pre><code>from cancelable import Cancellable\nimport asyncio\n\nasync def data_generator():\n    \"\"\"Simulate a data stream.\"\"\"\n    count = 0\n    while True:\n        await asyncio.sleep(0.1)\n        yield f\"data_packet_{count}\"\n        count += 1\n\nasync def process_stream():\n    async with Cancellable.with_timeout(5.0) as cancel:\n        async for packet in cancel.stream(data_generator()):\n            print(f\"Processing: {packet}\")\n            # Cancellation checked automatically between iterations\n</code></pre>"},{"location":"examples/stream_processing/#stream-with-progress-reporting","title":"Stream with Progress Reporting","text":"<pre><code>async def process_large_stream():\n    async with Cancellable.with_timeout(60.0) as cancel:\n        cancel.on_progress(\n            lambda op_id, msg, meta: print(f\"Stream progress: {msg}\")\n        )\n\n        async for item in cancel.stream(\n            fetch_items_from_api(),\n            report_interval=100  # Report every 100 items\n        ):\n            result = await transform_item(item)\n            await save_result(result)\n</code></pre>"},{"location":"examples/stream_processing/#buffered-stream-processing","title":"Buffered Stream Processing","text":""},{"location":"examples/stream_processing/#partial-results-recovery","title":"Partial Results Recovery","text":"<pre><code>async def recoverable_stream_processing():\n    \"\"\"Process stream with ability to recover partial results.\"\"\"\n\n    async with Cancellable.with_timeout(30.0) as cancel:\n        try:\n            processed_items = []\n\n            async for item in cancel.stream(\n                large_dataset_stream(),\n                buffer_partial=True  # Enable buffering\n            ):\n                # Process item\n                result = await complex_processing(item)\n                processed_items.append(result)\n\n                # Optionally save intermediate results\n                if len(processed_items) % 10 == 0:\n                    await save_checkpoint(processed_items)\n\n            return processed_items\n\n        except asyncio.CancelledError:\n            # Recover partial results\n            partial = cancel.context.partial_result\n            print(f\"Processed {partial['count']} items before cancellation\")\n\n            # Return what we processed so far\n            return processed_items\n</code></pre>"},{"location":"examples/stream_processing/#windowed-stream-processing","title":"Windowed Stream Processing","text":"<pre><code>async def windowed_stream_processing():\n    \"\"\"Process stream in fixed-size windows.\"\"\"\n\n    async def process_window(window: list):\n        # Process batch of items together\n        return await batch_transform(window)\n\n    async with Cancellable.with_timeout(120.0) as cancel:\n        window = []\n        window_size = 50\n\n        async for item in cancel.stream(real_time_feed()):\n            window.append(item)\n\n            if len(window) &gt;= window_size:\n                # Process full window\n                results = await process_window(window)\n                await output_results(results)\n\n                # Report progress\n                await cancel.report_progress(\n                    f\"Processed window of {window_size} items\",\n                    {\"window_count\": cancel.context.metadata.get(\"windows\", 0) + 1}\n                )\n\n                # Update metadata\n                cancel.context.metadata[\"windows\"] = cancel.context.metadata.get(\"windows\", 0) + 1\n\n                # Clear window\n                window = []\n\n        # Process remaining items\n        if window:\n            results = await process_window(window)\n            await output_results(results)\n</code></pre>"},{"location":"examples/stream_processing/#parallel-stream-processing","title":"Parallel Stream Processing","text":""},{"location":"examples/stream_processing/#fan-out-pattern","title":"Fan-Out Pattern","text":"<pre><code>from asyncio import Queue, create_task, gather\n\nasync def fan_out_processing():\n    \"\"\"Process stream items in parallel workers.\"\"\"\n\n    async def worker(worker_id: int, queue: Queue, cancel: Cancellable):\n        \"\"\"Process items from queue.\"\"\"\n        while True:\n            try:\n                # Get item with timeout\n                item = await asyncio.wait_for(\n                    queue.get(),\n                    timeout=1.0\n                )\n\n                # Process item\n                print(f\"Worker {worker_id} processing: {item}\")\n                result = await process_item(item)\n                await save_result(result)\n\n                queue.task_done()\n\n            except asyncio.TimeoutError:\n                # Check if we should stop\n                if queue.empty() and cancel.is_cancelled:\n                    break\n\n    async with Cancellable.with_timeout(60.0) as cancel:\n        # Create work queue\n        queue = Queue(maxsize=100)\n\n        # Start workers\n        workers = []\n        for i in range(5):\n            worker_task = create_task(worker(i, queue, cancel))\n            workers.append(worker_task)\n\n        # Feed items to queue\n        async for item in cancel.stream(data_source()):\n            await queue.put(item)\n\n        # Wait for queue to be processed\n        await queue.join()\n\n        # Cancel workers\n        await cancel.cancel()\n\n        # Wait for workers to finish\n        await gather(*workers, return_exceptions=True)\n</code></pre>"},{"location":"examples/stream_processing/#pipeline-pattern","title":"Pipeline Pattern","text":"<pre><code>async def stream_pipeline():\n    \"\"\"Multi-stage stream processing pipeline.\"\"\"\n\n    async def stage1_extract(stream):\n        \"\"\"Extract and parse raw data.\"\"\"\n        async for raw_data in stream:\n            parsed = await parse_data(raw_data)\n            if parsed:\n                yield parsed\n\n    async def stage2_transform(stream):\n        \"\"\"Transform parsed data.\"\"\"\n        async for parsed_data in stream:\n            transformed = await transform_data(parsed_data)\n            yield transformed\n\n    async def stage3_enrich(stream):\n        \"\"\"Enrich transformed data.\"\"\"\n        async for transformed_data in stream:\n            enriched = await enrich_with_metadata(transformed_data)\n            yield enriched\n\n    async with Cancellable.with_timeout(300.0) as cancel:\n        # Build pipeline\n        raw_stream = cancel.stream(fetch_raw_data())\n        parsed_stream = stage1_extract(raw_stream)\n        transformed_stream = stage2_transform(parsed_stream)\n        enriched_stream = stage3_enrich(transformed_stream)\n\n        # Process final stream\n        results = []\n        async for final_item in enriched_stream:\n            results.append(final_item)\n\n            if len(results) % 10 == 0:\n                await cancel.report_progress(\n                    f\"Pipeline processed {len(results)} items\"\n                )\n\n        return results\n</code></pre>"},{"location":"examples/stream_processing/#real-time-stream-processing","title":"Real-Time Stream Processing","text":""},{"location":"examples/stream_processing/#event-stream-processing","title":"Event Stream Processing","text":"<pre><code>async def process_event_stream():\n    \"\"\"Process real-time events with backpressure handling.\"\"\"\n\n    async with Cancellable.with_signal(signal.SIGINT) as cancel:\n        cancel.on_cancel(\n            lambda ctx: print(f\"Shutting down event processor: {ctx.cancel_reason}\")\n        )\n\n        # Track processing rate\n        events_processed = 0\n        start_time = asyncio.get_event_loop().time()\n\n        async for event in cancel.stream(event_source()):\n            # Process event\n            try:\n                await handle_event(event)\n                events_processed += 1\n\n                # Report metrics periodically\n                if events_processed % 1000 == 0:\n                    elapsed = asyncio.get_event_loop().time() - start_time\n                    rate = events_processed / elapsed\n\n                    await cancel.report_progress(\n                        f\"Processing rate: {rate:.2f} events/sec\",\n                        {\n                            \"events_processed\": events_processed,\n                            \"rate\": rate,\n                            \"elapsed\": elapsed\n                        }\n                    )\n\n            except Exception as e:\n                logger.error(f\"Error processing event: {e}\")\n                # Continue processing other events\n</code></pre>"},{"location":"examples/stream_processing/#websocket-stream-processing","title":"WebSocket Stream Processing","text":"<pre><code>import aiohttp\n\nasync def process_websocket_stream(url: str):\n    \"\"\"Process WebSocket messages with cancellation.\"\"\"\n\n    async with Cancellable.with_timeout(3600.0) as cancel:  # 1 hour timeout\n        session = aiohttp.ClientSession()\n\n        try:\n            async with session.ws_connect(url) as ws:\n                cancel.on_cancel(\n                    lambda ctx: asyncio.create_task(ws.close())\n                )\n\n                # Wrap WebSocket messages in cancellable stream\n                async def message_generator():\n                    async for msg in ws:\n                        if msg.type == aiohttp.WSMsgType.TEXT:\n                            yield msg.data\n                        elif msg.type == aiohttp.WSMsgType.ERROR:\n                            raise Exception(f\"WebSocket error: {ws.exception()}\")\n\n                # Process messages\n                async for message in cancel.stream(message_generator()):\n                    data = json.loads(message)\n                    await process_websocket_data(data)\n\n        finally:\n            await session.close()\n</code></pre>"},{"location":"examples/stream_processing/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"examples/stream_processing/#stateful-stream-processing","title":"Stateful Stream Processing","text":"<pre><code>class StreamAggregator:\n    \"\"\"Stateful stream processor with cancellation support.\"\"\"\n\n    def __init__(self):\n        self.state = {\n            \"count\": 0,\n            \"sum\": 0,\n            \"min\": float('inf'),\n            \"max\": float('-inf')\n        }\n\n    async def process_stream(self, data_stream, cancel_token):\n        \"\"\"Process stream while maintaining state.\"\"\"\n\n        async with Cancellable.with_token(cancel_token) as cancel:\n            cancel.on_cancel(\n                lambda ctx: print(f\"Aggregation stopped: {self.get_summary()}\")\n            )\n\n            async for value in cancel.stream(data_stream):\n                # Update state\n                self.state[\"count\"] += 1\n                self.state[\"sum\"] += value\n                self.state[\"min\"] = min(self.state[\"min\"], value)\n                self.state[\"max\"] = max(self.state[\"max\"], value)\n\n                # Emit periodic summaries\n                if self.state[\"count\"] % 100 == 0:\n                    await cancel.report_progress(\n                        \"Aggregation update\",\n                        self.get_summary()\n                    )\n\n    def get_summary(self):\n        \"\"\"Get current aggregation summary.\"\"\"\n        if self.state[\"count\"] == 0:\n            return {\"count\": 0}\n\n        return {\n            \"count\": self.state[\"count\"],\n            \"sum\": self.state[\"sum\"],\n            \"average\": self.state[\"sum\"] / self.state[\"count\"],\n            \"min\": self.state[\"min\"],\n            \"max\": self.state[\"max\"]\n        }\n</code></pre>"},{"location":"examples/stream_processing/#stream-joining","title":"Stream Joining","text":"<pre><code>async def join_streams():\n    \"\"\"Join multiple streams with cancellation.\"\"\"\n\n    async def merge_streams(*streams):\n        \"\"\"Merge multiple async iterators.\"\"\"\n        queue = asyncio.Queue()\n        finished = set()\n\n        async def consume_stream(stream_id, stream):\n            try:\n                async for item in stream:\n                    await queue.put((stream_id, item))\n            finally:\n                finished.add(stream_id)\n\n        # Start consumers\n        tasks = []\n        for i, stream in enumerate(streams):\n            task = asyncio.create_task(consume_stream(i, stream))\n            tasks.append(task)\n\n        # Yield merged items\n        while len(finished) &lt; len(streams) or not queue.empty():\n            try:\n                stream_id, item = await asyncio.wait_for(\n                    queue.get(), \n                    timeout=0.1\n                )\n                yield stream_id, item\n            except asyncio.TimeoutError:\n                continue\n\n        # Wait for all tasks\n        await asyncio.gather(*tasks)\n\n    async with Cancellable.with_timeout(60.0) as cancel:\n        # Create multiple streams\n        stream1 = cancel.stream(sensor_data_stream(\"sensor1\"))\n        stream2 = cancel.stream(sensor_data_stream(\"sensor2\"))\n        stream3 = cancel.stream(sensor_data_stream(\"sensor3\"))\n\n        # Process merged stream\n        async for stream_id, data in merge_streams(stream1, stream2, stream3):\n            print(f\"Stream {stream_id}: {data}\")\n            # Process combined data\n            await process_sensor_reading(stream_id, data)\n</code></pre>"},{"location":"examples/stream_processing/#error-handling-in-streams","title":"Error Handling in Streams","text":""},{"location":"examples/stream_processing/#resilient-stream-processing","title":"Resilient Stream Processing","text":"<pre><code>async def resilient_stream_processing():\n    \"\"\"Process stream with error recovery.\"\"\"\n\n    async def process_with_retry(item, max_retries=3):\n        \"\"\"Process item with retry logic.\"\"\"\n        for attempt in range(max_retries):\n            try:\n                return await process_item(item)\n            except Exception as e:\n                if attempt == max_retries - 1:\n                    logger.error(f\"Failed to process {item} after {max_retries} attempts\")\n                    raise\n                await asyncio.sleep(0.5 * (attempt + 1))  # Exponential backoff\n\n    async with Cancellable.with_timeout(300.0) as cancel:\n        failed_items = []\n        successful_count = 0\n\n        async for item in cancel.stream(unreliable_data_source()):\n            try:\n                result = await process_with_retry(item)\n                successful_count += 1\n\n                # Save result\n                await save_result(result)\n\n            except Exception as e:\n                failed_items.append({\n                    \"item\": item,\n                    \"error\": str(e),\n                    \"timestamp\": asyncio.get_event_loop().time()\n                })\n\n                # Report failure\n                await cancel.report_progress(\n                    f\"Processing status\",\n                    {\n                        \"successful\": successful_count,\n                        \"failed\": len(failed_items),\n                        \"total\": successful_count + len(failed_items)\n                    }\n                )\n\n        # Return summary\n        return {\n            \"successful\": successful_count,\n            \"failed\": failed_items\n        }\n</code></pre>"},{"location":"examples/stream_processing/#performance-optimization","title":"Performance Optimization","text":""},{"location":"examples/stream_processing/#chunked-processing","title":"Chunked Processing","text":"<pre><code>async def optimized_bulk_processing():\n    \"\"\"Process stream in optimized chunks.\"\"\"\n\n    async def process_chunk(chunk):\n        \"\"\"Process multiple items efficiently.\"\"\"\n        # Bulk operation (e.g., database insert)\n        return await bulk_insert(chunk)\n\n    async with Cancellable.with_timeout(600.0) as cancel:\n        chunk = []\n        chunk_size = 1000\n        total_processed = 0\n\n        async for item in cancel.stream(large_dataset()):\n            chunk.append(item)\n\n            if len(chunk) &gt;= chunk_size:\n                # Process chunk\n                await process_chunk(chunk)\n                total_processed += len(chunk)\n\n                # Report progress\n                await cancel.report_progress(\n                    f\"Processed {total_processed} items\",\n                    {\n                        \"total\": total_processed,\n                        \"chunks\": total_processed // chunk_size,\n                        \"rate\": total_processed / cancel.context.duration\n                    }\n                )\n\n                # Clear chunk\n                chunk = []\n\n        # Process remaining items\n        if chunk:\n            await process_chunk(chunk)\n            total_processed += len(chunk)\n\n        return total_processed\n</code></pre>"},{"location":"examples/stream_processing/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate buffer sizes: Balance memory usage with performance</li> <li>Report progress regularly: Keep users informed during long operations</li> <li>Handle backpressure: Use queues with size limits for parallel processing</li> <li>Process in chunks: For better performance with bulk operations</li> <li>Implement error recovery: Don't let single failures stop entire stream</li> <li>Clean up resources: Ensure streams are properly closed on cancellation</li> <li>Monitor performance: Track processing rates and adjust accordingly</li> </ol>"},{"location":"examples/stream_processing/#next-steps","title":"Next Steps","text":"<ul> <li>Review the API Reference for detailed documentation</li> <li>Explore Integration examples for specific use cases</li> <li>Check out Patterns for more advanced patterns</li> </ul>"},{"location":"integrations/fastapi/","title":"FastAPI Integration","text":"<p>The cancelable library provides seamless integration with FastAPI for building cancellable endpoints.</p>"},{"location":"integrations/fastapi/#installation","title":"Installation","text":"<p>The FastAPI integration is included when you install cancelable:</p> <pre><code>uv add cancelable\n</code></pre>"},{"location":"integrations/fastapi/#basic-usage","title":"Basic Usage","text":""},{"location":"integrations/fastapi/#dependency-injection","title":"Dependency Injection","text":"<pre><code>from fastapi import FastAPI, Depends\nfrom cancelable.integrations.fastapi import cancellable_dependency\nfrom cancelable import Cancellable\n\napp = FastAPI()\n\n@app.get(\"/long-operation\")\nasync def long_operation(cancel: Cancellable = Depends(cancellable_dependency)):\n    async with cancel:\n        # Your long-running operation here\n        result = await process_data()\n        return {\"result\": result}\n</code></pre>"},{"location":"integrations/fastapi/#timeout-configuration","title":"Timeout Configuration","text":"<pre><code>from cancelable.integrations.fastapi import cancellable_dependency\n\n# Create a custom dependency with timeout\ndef get_cancellable_30s():\n    return cancellable_dependency(timeout=30.0)\n\n@app.get(\"/timeout-operation\")\nasync def timeout_operation(cancel: Cancellable = Depends(get_cancellable_30s)):\n    async with cancel:\n        # This operation will timeout after 30 seconds\n        return await long_computation()\n</code></pre>"},{"location":"integrations/fastapi/#advanced-usage","title":"Advanced Usage","text":""},{"location":"integrations/fastapi/#background-tasks-with-cancellation","title":"Background Tasks with Cancellation","text":"<pre><code>from fastapi import BackgroundTasks\nfrom cancelable import CancellationToken\n\n# Store tokens for background tasks\nbackground_tokens = {}\n\n@app.post(\"/start-background-task\")\nasync def start_background_task(background_tasks: BackgroundTasks):\n    token = CancellationToken()\n    task_id = str(uuid.uuid4())\n    background_tokens[task_id] = token\n\n    background_tasks.add_task(\n        background_worker,\n        task_id,\n        token\n    )\n\n    return {\"task_id\": task_id}\n\nasync def background_worker(task_id: str, token: CancellationToken):\n    async with Cancellable.with_token(token) as cancel:\n        try:\n            # Long-running background work\n            await process_large_dataset()\n        finally:\n            # Cleanup\n            background_tokens.pop(task_id, None)\n\n@app.post(\"/cancel-task/{task_id}\")\nasync def cancel_task(task_id: str):\n    token = background_tokens.get(task_id)\n    if token:\n        await token.cancel()\n        return {\"status\": \"cancelled\"}\n    return {\"status\": \"not_found\"}\n</code></pre>"},{"location":"integrations/fastapi/#request-cancellation-handling","title":"Request Cancellation Handling","text":"<pre><code>from starlette.requests import Request\n\n@app.get(\"/streaming-response\")\nasync def streaming_response(\n    request: Request,\n    cancel: Cancellable = Depends(cancellable_dependency)\n):\n    async def generate():\n        async with cancel:\n            for i in range(100):\n                # Check if client disconnected\n                if await request.is_disconnected():\n                    await cancel.cancel()\n                    break\n\n                yield f\"data: Item {i}\\n\\n\"\n                await asyncio.sleep(0.1)\n\n    return StreamingResponse(\n        generate(),\n        media_type=\"text/event-stream\"\n    )\n</code></pre>"},{"location":"integrations/fastapi/#best-practices","title":"Best Practices","text":"<ol> <li>Always use async with: Ensure proper cleanup by using the context manager</li> <li>Set appropriate timeouts: Configure timeouts based on expected operation duration</li> <li>Handle cancellation gracefully: Catch CancelledError and perform cleanup</li> <li>Monitor operations: Use the global registry for debugging and monitoring</li> </ol>"},{"location":"integrations/fastapi/#example-file-upload-with-progress","title":"Example: File Upload with Progress","text":"<pre><code>from cancelable import Cancellable\nfrom fastapi import UploadFile, File\n\n@app.post(\"/upload\")\nasync def upload_file(\n    file: UploadFile = File(...),\n    cancel: Cancellable = Depends(cancellable_dependency)\n):\n    async with cancel:\n        cancel.on_progress(\n            lambda op_id, msg, meta: logger.info(f\"Upload progress: {msg}\")\n        )\n\n        total_size = 0\n        chunk_size = 1024 * 1024  # 1MB chunks\n\n        with open(f\"uploads/{file.filename}\", \"wb\") as f:\n            while chunk := await file.read(chunk_size):\n                f.write(chunk)\n                total_size += len(chunk)\n\n                await cancel.report_progress(\n                    f\"Uploaded {total_size} bytes\",\n                    {\"bytes\": total_size, \"filename\": file.filename}\n                )\n\n        return {\n            \"filename\": file.filename,\n            \"size\": total_size,\n            \"status\": \"completed\"\n        }\n</code></pre>"},{"location":"integrations/fastapi/#error-handling","title":"Error Handling","text":"<pre><code>from cancelable import TimeoutCancellation\n\n@app.get(\"/with-error-handling\")\nasync def with_error_handling(cancel: Cancellable = Depends(cancellable_dependency)):\n    try:\n        async with cancel:\n            result = await some_operation()\n            return {\"result\": result}\n    except TimeoutCancellation:\n        return JSONResponse(\n            status_code=408,\n            content={\"error\": \"Operation timed out\"}\n        )\n    except Exception as e:\n        return JSONResponse(\n            status_code=500,\n            content={\"error\": str(e)}\n        )\n</code></pre>"},{"location":"integrations/httpx/","title":"HTTPX Integration","text":"<p>The cancelable library provides a cancellable HTTP client built on top of HTTPX, enabling you to cancel HTTP requests and handle timeouts gracefully.</p>"},{"location":"integrations/httpx/#installation","title":"Installation","text":"<p>The HTTPX integration is included when you install cancelable:</p> <pre><code>uv add cancelable\n</code></pre>"},{"location":"integrations/httpx/#basic-usage","title":"Basic Usage","text":""},{"location":"integrations/httpx/#cancellable-http-client","title":"Cancellable HTTP Client","text":"<pre><code>from cancelable import Cancellable\nfrom cancelable.integrations.httpx import CancellableHTTPClient\n\nasync with Cancellable.with_timeout(30.0) as cancel:\n    async with CancellableHTTPClient(cancel) as client:\n        response = await client.get(\"https://api.example.com/data\")\n        data = response.json()\n</code></pre>"},{"location":"integrations/httpx/#file-downloads-with-progress","title":"File Downloads with Progress","text":"<pre><code>from cancelable.integrations.httpx import download_file\n\nasync with Cancellable.with_timeout(300.0) as cancel:\n    cancel.on_progress(\n        lambda op_id, msg, meta: print(f\"Download: {msg}\")\n    )\n\n    bytes_downloaded = await download_file(\n        \"https://example.com/large-file.zip\",\n        \"/tmp/large-file.zip\",\n        cancel,\n        resume=True  # Enable resume support\n    )\n</code></pre>"},{"location":"integrations/httpx/#advanced-features","title":"Advanced Features","text":""},{"location":"integrations/httpx/#request-cancellation","title":"Request Cancellation","text":"<pre><code>from cancelable import CancellationToken\n\n# Manual cancellation\ntoken = CancellationToken()\n\nasync def download_task():\n    async with Cancellable.with_token(token) as cancel:\n        async with CancellableHTTPClient(cancel) as client:\n            # This request can be cancelled by calling token.cancel()\n            response = await client.get(\"https://example.com/slow-endpoint\")\n            return response.json()\n\n# In another task/thread\nawait token.cancel(\"User requested cancellation\")\n</code></pre>"},{"location":"integrations/httpx/#streaming-responses","title":"Streaming Responses","text":"<pre><code>async with Cancellable.with_timeout(60.0) as cancel:\n    async with CancellableHTTPClient(cancel) as client:\n        async with client.stream(\"GET\", \"https://example.com/stream\") as response:\n            async for chunk in response.aiter_bytes(chunk_size=1024):\n                # Process each chunk\n                await process_chunk(chunk)\n\n                # Cancellation is checked automatically between chunks\n</code></pre>"},{"location":"integrations/httpx/#multiple-requests-with-shared-cancellation","title":"Multiple Requests with Shared Cancellation","text":"<pre><code>async with Cancellable.with_timeout(30.0) as cancel:\n    async with CancellableHTTPClient(cancel) as client:\n        # All requests share the same cancellation context\n        responses = await asyncio.gather(\n            client.get(\"https://api1.example.com/data\"),\n            client.get(\"https://api2.example.com/data\"),\n            client.get(\"https://api3.example.com/data\"),\n        )\n</code></pre>"},{"location":"integrations/httpx/#configuration-options","title":"Configuration Options","text":""},{"location":"integrations/httpx/#custom-httpx-client-options","title":"Custom HTTPX Client Options","text":"<pre><code>from httpx import Limits\n\nasync with Cancellable.with_timeout(30.0) as cancel:\n    async with CancellableHTTPClient(\n        cancel,\n        # All standard HTTPX client options are supported\n        base_url=\"https://api.example.com\",\n        headers={\"Authorization\": \"Bearer token\"},\n        limits=Limits(max_keepalive_connections=10),\n        http2=True,\n    ) as client:\n        response = await client.get(\"/endpoint\")\n</code></pre>"},{"location":"integrations/httpx/#progress-tracking-for-large-downloads","title":"Progress Tracking for Large Downloads","text":"<pre><code>async def download_with_detailed_progress(url: str, path: str):\n    async with Cancellable.with_timeout(600.0) as cancel:\n        cancel.on_progress(\n            lambda op_id, msg, meta: logger.info(\n                f\"Download progress\",\n                url=url,\n                **meta\n            )\n        )\n\n        total_bytes = await download_file(\n            url,\n            path,\n            cancel,\n            chunk_size=1024 * 1024,  # 1MB chunks\n            report_interval=10,  # Report every 10 chunks\n        )\n\n        return total_bytes\n</code></pre>"},{"location":"integrations/httpx/#error-handling","title":"Error Handling","text":""},{"location":"integrations/httpx/#handling-different-cancellation-scenarios","title":"Handling Different Cancellation Scenarios","text":"<pre><code>from cancelable import TimeoutCancellation, ManualCancellation\nfrom httpx import HTTPError\n\nasync def resilient_request(url: str):\n    try:\n        async with Cancellable.with_timeout(30.0) as cancel:\n            async with CancellableHTTPClient(cancel) as client:\n                response = await client.get(url)\n                return response.json()\n\n    except TimeoutCancellation:\n        logger.error(f\"Request timed out: {url}\")\n        raise\n\n    except ManualCancellation as e:\n        logger.info(f\"Request cancelled: {e.message}\")\n        raise\n\n    except HTTPError as e:\n        logger.error(f\"HTTP error: {e}\")\n        raise\n</code></pre>"},{"location":"integrations/httpx/#best-practices","title":"Best Practices","text":"<ol> <li>Set appropriate timeouts: Different endpoints may need different timeout values</li> <li>Use connection pooling: Reuse the client for multiple requests when possible</li> <li>Handle partial downloads: Use the resume feature for large file downloads</li> <li>Monitor progress: Attach progress callbacks for long-running downloads</li> </ol>"},{"location":"integrations/httpx/#example-parallel-downloads-with-cancellation","title":"Example: Parallel Downloads with Cancellation","text":"<pre><code>from cancelable import Cancellable, CancellationToken\n\nasync def download_multiple_files(urls: list[str], cancel_token: CancellationToken):\n    \"\"\"Download multiple files with shared cancellation.\"\"\"\n\n    async with Cancellable.with_token(cancel_token) as cancel:\n        cancel.on_progress(\n            lambda op_id, msg, meta: logger.info(f\"{msg}\", **meta)\n        )\n\n        tasks = []\n        for i, url in enumerate(urls):\n            filename = f\"download_{i}.dat\"\n            task = download_file(url, filename, cancel)\n            tasks.append(task)\n\n        # All downloads share the same cancellation context\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        # Process results\n        successful = sum(1 for r in results if not isinstance(r, Exception))\n        logger.info(f\"Downloaded {successful}/{len(urls)} files successfully\")\n\n        return results\n</code></pre>"},{"location":"integrations/httpx/#integration-with-retry-logic","title":"Integration with Retry Logic","text":"<pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=1, max=10)\n)\nasync def download_with_retry(url: str, path: str):\n    async with Cancellable.with_timeout(60.0) as cancel:\n        return await download_file(url, path, cancel)\n</code></pre>"},{"location":"integrations/sqlalchemy/","title":"SQLAlchemy Integration","text":"<p>The cancelable library provides integration with SQLAlchemy for cancellable database operations, supporting both async and sync database sessions.</p>"},{"location":"integrations/sqlalchemy/#installation","title":"Installation","text":"<p>The SQLAlchemy integration is included when you install cancelable:</p> <pre><code>uv add cancelable\n</code></pre>"},{"location":"integrations/sqlalchemy/#basic-usage","title":"Basic Usage","text":""},{"location":"integrations/sqlalchemy/#cancellable-database-session","title":"Cancellable Database Session","text":"<pre><code>from sqlalchemy.ext.asyncio import create_async_engine\nfrom cancelable import Cancellable\nfrom cancelable.integrations.sqlalchemy import cancellable_session\n\n# Create async engine\nengine = create_async_engine(\"sqlite+aiosqlite:///example.db\")\n\nasync with Cancellable.with_timeout(30.0) as cancel:\n    async with cancellable_session(engine, cancel) as session:\n        # Execute queries with cancellation support\n        result = await session.execute(\n            select(User).where(User.active == True)\n        )\n        users = result.scalars().all()\n</code></pre>"},{"location":"integrations/sqlalchemy/#long-running-queries","title":"Long-Running Queries","text":"<pre><code>async def process_large_dataset():\n    async with Cancellable.with_timeout(300.0) as cancel:\n        cancel.on_progress(\n            lambda op_id, msg, meta: logger.info(f\"Query progress: {msg}\")\n        )\n\n        async with cancellable_session(engine, cancel) as session:\n            # Process in batches with cancellation checks\n            offset = 0\n            batch_size = 1000\n            total_processed = 0\n\n            while True:\n                result = await session.execute(\n                    select(Order)\n                    .limit(batch_size)\n                    .offset(offset)\n                )\n\n                orders = result.scalars().all()\n                if not orders:\n                    break\n\n                # Process batch\n                for order in orders:\n                    await process_order(order)\n                    total_processed += 1\n\n                # Report progress\n                await cancel.report_progress(\n                    f\"Processed {total_processed} orders\",\n                    {\"count\": total_processed, \"batch\": offset // batch_size}\n                )\n\n                offset += batch_size\n\n                # Commit after each batch\n                await session.commit()\n</code></pre>"},{"location":"integrations/sqlalchemy/#advanced-features","title":"Advanced Features","text":""},{"location":"integrations/sqlalchemy/#transaction-management-with-cancellation","title":"Transaction Management with Cancellation","text":"<pre><code>async with Cancellable.with_timeout(60.0) as cancel:\n    async with cancellable_session(engine, cancel) as session:\n        try:\n            # Start transaction\n            async with session.begin():\n                # Multiple operations in transaction\n                user = User(name=\"John Doe\", email=\"john@example.com\")\n                session.add(user)\n\n                # Simulate long operation\n                await cancel.report_progress(\"Creating user account\")\n\n                profile = UserProfile(user_id=user.id, bio=\"New user\")\n                session.add(profile)\n\n                # Transaction automatically commits on success\n\n        except CancelledError:\n            # Transaction automatically rolls back on cancellation\n            logger.warning(\"Transaction cancelled, rolling back\")\n            raise\n</code></pre>"},{"location":"integrations/sqlalchemy/#streaming-large-results","title":"Streaming Large Results","text":"<pre><code>async def stream_query_results():\n    async with Cancellable.with_timeout(120.0) as cancel:\n        async with cancellable_session(engine, cancel) as session:\n            # Use stream_scalars for memory-efficient processing\n            async with session.stream_scalars(\n                select(LogEntry).order_by(LogEntry.timestamp)\n            ) as result:\n                async for log_entry in result:\n                    # Process each row individually\n                    await process_log_entry(log_entry)\n\n                    # Cancellation is checked between iterations\n</code></pre>"},{"location":"integrations/sqlalchemy/#bulk-operations-with-progress","title":"Bulk Operations with Progress","text":"<pre><code>async def bulk_insert_with_cancellation(items: list[dict]):\n    async with Cancellable.with_timeout(300.0) as cancel:\n        cancel.on_progress(\n            lambda op_id, msg, meta: print(f\"Bulk insert: {msg}\")\n        )\n\n        async with cancellable_session(engine, cancel) as session:\n            # Process in chunks\n            chunk_size = 1000\n            total_inserted = 0\n\n            for i in range(0, len(items), chunk_size):\n                chunk = items[i:i + chunk_size]\n\n                # Bulk insert\n                await session.execute(\n                    insert(Product),\n                    chunk\n                )\n\n                total_inserted += len(chunk)\n\n                # Report progress\n                await cancel.report_progress(\n                    f\"Inserted {total_inserted}/{len(items)} items\",\n                    {\n                        \"inserted\": total_inserted,\n                        \"total\": len(items),\n                        \"percent\": (total_inserted / len(items)) * 100\n                    }\n                )\n\n                # Commit after each chunk\n                await session.commit()\n</code></pre>"},{"location":"integrations/sqlalchemy/#connection-pool-management","title":"Connection Pool Management","text":""},{"location":"integrations/sqlalchemy/#cancellable-pool-configuration","title":"Cancellable Pool Configuration","text":"<pre><code>from sqlalchemy.pool import NullPool\n\n# Create engine with custom pool settings\nengine = create_async_engine(\n    \"postgresql+asyncpg://user:pass@localhost/db\",\n    pool_pre_ping=True,  # Test connections before use\n    pool_size=20,\n    max_overflow=10,\n    pool_timeout=30,  # Wait max 30s for connection\n)\n\nasync def get_connection_with_cancel():\n    async with Cancellable.with_timeout(5.0) as cancel:\n        # Cancel if can't get connection within 5 seconds\n        async with cancellable_session(engine, cancel) as session:\n            return await session.execute(select(1))\n</code></pre>"},{"location":"integrations/sqlalchemy/#error-handling","title":"Error Handling","text":""},{"location":"integrations/sqlalchemy/#handling-database-errors-with-cancellation","title":"Handling Database Errors with Cancellation","text":"<pre><code>from sqlalchemy.exc import DBAPIError, IntegrityError\nfrom cancelable import TimeoutCancellation\n\nasync def safe_database_operation():\n    try:\n        async with Cancellable.with_timeout(30.0) as cancel:\n            async with cancellable_session(engine, cancel) as session:\n                # Database operations\n                result = await session.execute(query)\n                await session.commit()\n                return result\n\n    except TimeoutCancellation:\n        logger.error(\"Database operation timed out\")\n        # Handle timeout appropriately\n        raise\n\n    except IntegrityError as e:\n        logger.error(f\"Integrity constraint violated: {e}\")\n        # Handle constraint violations\n        raise\n\n    except DBAPIError as e:\n        logger.error(f\"Database error: {e}\")\n        # Handle other database errors\n        raise\n</code></pre>"},{"location":"integrations/sqlalchemy/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate timeouts: Set timeouts based on expected query duration</li> <li>Process in batches: For large datasets, process in manageable chunks</li> <li>Check cancellation regularly: In long loops, ensure cancellation is checked</li> <li>Clean up resources: Sessions are automatically cleaned up on cancellation</li> <li>Use streaming for large results: Prevent memory issues with stream_scalars</li> </ol>"},{"location":"integrations/sqlalchemy/#example-etl-pipeline-with-cancellation","title":"Example: ETL Pipeline with Cancellation","text":"<pre><code>async def etl_pipeline(source_engine, target_engine, cancel_token):\n    \"\"\"Extract, transform, and load data with cancellation support.\"\"\"\n\n    async with Cancellable.with_token(cancel_token) as cancel:\n        cancel.on_progress(\n            lambda op_id, msg, meta: logger.info(f\"ETL: {msg}\", **meta)\n        )\n\n        # Extract\n        await cancel.report_progress(\"Starting extraction\")\n        async with cancellable_session(source_engine, cancel) as source_session:\n            data = await source_session.execute(\n                select(SourceTable).where(SourceTable.processed == False)\n            )\n            records = data.scalars().all()\n\n        await cancel.report_progress(f\"Extracted {len(records)} records\")\n\n        # Transform\n        await cancel.report_progress(\"Starting transformation\")\n        transformed = []\n        for i, record in enumerate(records):\n            transformed_record = await transform_record(record)\n            transformed.append(transformed_record)\n\n            if i % 100 == 0:\n                await cancel.report_progress(\n                    f\"Transformed {i}/{len(records)} records\"\n                )\n\n        # Load\n        await cancel.report_progress(\"Starting load\")\n        async with cancellable_session(target_engine, cancel) as target_session:\n            # Bulk insert transformed data\n            await target_session.execute(\n                insert(TargetTable),\n                transformed\n            )\n            await target_session.commit()\n\n        await cancel.report_progress(f\"ETL completed: {len(records)} records processed\")\n</code></pre>"},{"location":"integrations/sqlalchemy/#integration-with-alembic-migrations","title":"Integration with Alembic Migrations","text":"<pre><code>from alembic import command\nfrom alembic.config import Config\n\nasync def run_migrations_with_timeout():\n    \"\"\"Run database migrations with timeout.\"\"\"\n\n    async with Cancellable.with_timeout(300.0) as cancel:\n        cancel.on_progress(\n            lambda op_id, msg, meta: logger.info(f\"Migration: {msg}\")\n        )\n\n        # Note: Alembic migrations are synchronous\n        # We can still use cancellation for the overall timeout\n        alembic_cfg = Config(\"alembic.ini\")\n\n        await cancel.report_progress(\"Starting migrations\")\n\n        # Run in thread pool to avoid blocking\n        await asyncio.to_thread(\n            command.upgrade,\n            alembic_cfg,\n            \"head\"\n        )\n\n        await cancel.report_progress(\"Migrations completed\")\n</code></pre>"}]}